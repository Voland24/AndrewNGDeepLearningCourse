{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+gAEH3szqGEN1NU/YbCf1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Voland24/AndrewNGDeepLearningCourse/blob/main/Week2GradCheck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper methods provided by the course\n"
      ],
      "metadata": {
        "id": "sOYxYdRFCBDg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of x\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- sigmoid(x)\n",
        "    \"\"\"\n",
        "    s = 1/(1+np.exp(-x))\n",
        "    return s\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    Compute the relu of x\n",
        "    Arguments:\n",
        "    x -- A scalar or numpy array of any size.\n",
        "    Return:\n",
        "    s -- relu(x)\n",
        "    \"\"\"\n",
        "    s = np.maximum(0,x)\n",
        "    \n",
        "    return s\n",
        "\n",
        "def dictionary_to_vector(parameters):\n",
        "    \"\"\"\n",
        "    Roll all our parameters dictionary into a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    keys = []\n",
        "    count = 0\n",
        "    for key in [\"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"]:\n",
        "        \n",
        "        # flatten parameter\n",
        "        new_vector = np.reshape(parameters[key], (-1,1))\n",
        "        keys = keys + [key]*new_vector.shape[0]\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = new_vector\n",
        "        else:\n",
        "            theta = np.concatenate((theta, new_vector), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta, keys\n",
        "\n",
        "def vector_to_dictionary(theta):\n",
        "    \"\"\"\n",
        "    Unroll all our parameters dictionary from a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    parameters = {}\n",
        "    parameters[\"W1\"] = theta[:20].reshape((5,4))\n",
        "    parameters[\"b1\"] = theta[20:25].reshape((5,1))\n",
        "    parameters[\"W2\"] = theta[25:40].reshape((3,5))\n",
        "    parameters[\"b2\"] = theta[40:43].reshape((3,1))\n",
        "    parameters[\"W3\"] = theta[43:46].reshape((1,3))\n",
        "    parameters[\"b3\"] = theta[46:47].reshape((1,1))\n",
        "\n",
        "    return parameters\n",
        "\n",
        "def gradients_to_vector(gradients):\n",
        "    \"\"\"\n",
        "    Roll all our gradients dictionary into a single vector satisfying our specific required shape.\n",
        "    \"\"\"\n",
        "    \n",
        "    count = 0\n",
        "    for key in [\"dW1\", \"db1\", \"dW2\", \"db2\", \"dW3\", \"db3\"]:\n",
        "        # flatten parameter\n",
        "        new_vector = np.reshape(gradients[key], (-1,1))\n",
        "        \n",
        "        if count == 0:\n",
        "            theta = new_vector\n",
        "        else:\n",
        "            theta = np.concatenate((theta, new_vector), axis=0)\n",
        "        count = count + 1\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "Do9gDJUMB8Qy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The idea here is that we confirm that the gradient we are calculating with the chain rule i.e. backwards propagating of the gradient is approximately the same as the analytic gradient as well to a certain degree of accuracy"
      ],
      "metadata": {
        "id": "1wN7UV21EItR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case of 1D grad check"
      ],
      "metadata": {
        "id": "0MKAYpt7Eu0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(x, theta):\n",
        "  J = np.dot(theta, x)\n",
        "  return J"
      ],
      "metadata": {
        "id": "K1Am8YR2EY8Z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(x, theta):\n",
        "  dtheta = x\n",
        "  return dtheta"
      ],
      "metadata": {
        "id": "jagwA5jWEldY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check(x, theta, epsilon = 1e-7):\n",
        "  thetaplus = theta + epsilon\n",
        "  thetaminus = theta - epsilon\n",
        "  J_plus = forward_propagation(x, thetaplus)\n",
        "  J_minus = forward_propagation(x, thetaminus)\n",
        "  gradapprox = (J_plus - J_minus) / (2 * epsilon)\n",
        "\n",
        "  grad = backward_propagation(x, theta)\n",
        "\n",
        "  numerator = np.linalg.norm(grad - gradapprox)\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)\n",
        "  difference = numerator / denominator\n",
        "\n",
        "  if difference < 1e-7:\n",
        "    print(\"Grad is correct\")\n",
        "  else:\n",
        "    print(\"Grad is NOT correct\")\n",
        "  \n",
        "  return difference"
      ],
      "metadata": {
        "id": "EFadzUXqE15M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-dim grad check"
      ],
      "metadata": {
        "id": "EnuB9D4-FwIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation_n(X, Y, params):\n",
        "  \n",
        "  m = X.shape[1]\n",
        "  W1 = params[\"W1\"]\n",
        "  b1 = params[\"b1\"]\n",
        "  W2 = params[\"W2\"]\n",
        "  b2 = params[\"b2\"]\n",
        "  W3 = params[\"W3\"]\n",
        "  b3 = params[\"b3\"]\n",
        "\n",
        "  Z1 = np.dot(W1,X) + b1\n",
        "  A1 = relu(Z1)\n",
        "  Z2 = np.dot(W2, A1) + b2\n",
        "  A2 = relu(Z2)\n",
        "  Z3 = np.dot(W3,A2) + b3\n",
        "  A3 = sigmoid(Z3)\n",
        "\n",
        "  logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)\n",
        "  cost = (1./ m) * np.sum(logprobs)\n",
        "  cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)\n",
        "\n",
        "  return cost, cache\n"
      ],
      "metadata": {
        "id": "AD6LNGXiFzJQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation_n(X, Y , cache):\n",
        "  \n",
        "  m = X.shape[1]\n",
        "  (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
        "\n",
        "  dZ2 = A3 - Y\n",
        "  dW3 = (1. / m) * np.dot(dZ3, A2.T)\n",
        "  db3 = (1./m) * np.sum(dZ3, axis = 1, keepdims = True)\n",
        "\n",
        "  dA2 = np.dot(W3.T, dZ3)\n",
        "  dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
        "  dW2 = (1./m) * np.dot(dZ2, A1.T)\n",
        "  db2 = (1./m) * np.sum(dZ2, axis = 1, keepdims = True)\n",
        "\n",
        "  dA1 = np.dot(W2.T, dZ2)\n",
        "  dZ1 = np.multiply(dA1, np.int64(A2 > 0))\n",
        "  dW2 = (1./m) * np.dot(dZ1, X1.T)\n",
        "  db1 = 4./m * np.sum(dZ1, axis = 1, keepdims = True)\n",
        "\n",
        "  gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\n",
        "                 \"dA2\": dA2, \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2,\n",
        "                 \"dA1\": dA1, \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
        "\n",
        "  return gradients\n"
      ],
      "metadata": {
        "id": "So7sT4sOGw1b"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_check_n(params, gradients, X, Y, epsilon = 1e-7):\n",
        "\n",
        "  params_values, _ = dictionary_to_vector(params)\n",
        "  grad = gradients_to_vector(gradients)\n",
        "  num_params = params_values.shape[0]\n",
        "  J_plus = np.zeros((num_params, 1))\n",
        "  J_minus = np.zeros((num_params, 1))\n",
        "\n",
        "  for i in range(num_params):\n",
        "    thetaplus = np.copy(params_values)\n",
        "    thetaplus[i][0] += epsilon\n",
        "    J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))\n",
        "\n",
        "    thetaminus - np.copy(params_values)\n",
        "    thetaminus[i][0] -= epslion\n",
        "    J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))\n",
        "\n",
        "    gradapprox[i] = (J_plus[i] - J_minus[i]) / (2 * epsilon)\n",
        "  \n",
        "  numerator = np.linalg.norm(grad - gradapprox)                                     # Step 1'\n",
        "  denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)                   # Step 2'\n",
        "  difference = numerator / denominator                                              # Step 3'\n",
        "  ### END CODE HERE ###\n",
        "\n",
        "  if difference > 1e-7:\n",
        "      print(\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
        "  else:\n",
        "      print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")\n",
        "  \n",
        "  return difference\n"
      ],
      "metadata": {
        "id": "_ZEQYwAlH3md"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}